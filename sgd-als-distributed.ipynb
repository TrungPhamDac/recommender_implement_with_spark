{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8889/'. Verify the server is running and reachable. (request to http://localhost:8889/api/kernels?1669513101853 failed, reason: connect ECONNREFUSED 127.0.0.1:8889)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from pyspark import SparkConf, SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowAccumulatorParam(AccumulatorParam):\n",
    "    '''accumulator to add values to matrix row-wise'''\n",
    "    def zero(self, values):\n",
    "        return np.zeros(len(values)).astype('float16')\n",
    "    def addInPlace(self, value1, value2):\n",
    "        # value2 is tuple\n",
    "        # with i-th row to updated as key and\n",
    "        # value = vector of gradient descent updates\n",
    "        (i, w_i) = value2\n",
    "        value1[i] += w_i\n",
    "        return value1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColAccumulatorParam(AccumulatorParam):\n",
    "    '''accumulator to add values to matrix column-wise'''\n",
    "    def zero(self, values):\n",
    "        return np.zeros(len(values)).astype('float16')\n",
    "    def addInPlace(self, value1, value2):\n",
    "        # value2 is tuple\n",
    "        # with i-th row to updated as key and\n",
    "        # value = vector of gradient descent updates\n",
    "        (j, h_j) = value2\n",
    "        value1[:,j] += h_j\n",
    "        return value1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nans(x):\n",
    "    global nanchecker\n",
    "    if np.isnan(x[1]).any():\n",
    "        nanchecker += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_w(x):\n",
    "    global w_accum\n",
    "    w_accum += 1\n",
    "\n",
    "# action to call on H to cause update to be calculated (without collect())\n",
    "def coalesce_h(x):\n",
    "    global h_accum\n",
    "    h_accum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_row_block(i):\n",
    "    '''takes in row index and assigns to block'''\n",
    "    global row_block_size\n",
    "    # return block number from 0 to num_workers-1 (i.e. 0-95)\n",
    "    return np.floor(i/row_block_size).astype(int)\n",
    "\n",
    "def assign_col_block(j):\n",
    "    '''takes in column index and assigns to block'''\n",
    "    global col_block_size\n",
    "    # return block number from 0 to num_workers-1 (i.e. 0-95)\n",
    "    return np.floor(j/col_block_size).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x):\n",
    "    global n_updates_acc\n",
    "    global mse\n",
    "    for val in x:\n",
    "        row_block_id = val[0]\n",
    "        v_iter = val[1][0]\n",
    "        w_iter = val[1][1]\n",
    "        h_iter = val[1][2]\n",
    "    # dictionaries to store W and H\n",
    "    w = {xw[0]:xw[1] for xw in w_iter}\n",
    "    h = {xh[0]:xh[1] for xh in h_iter}\n",
    "    # go through V and update W and H\n",
    "    for v_ij in v_iter:\n",
    "        i, j = v_ij\n",
    "        # get row and column\n",
    "        w_i = w[i]\n",
    "        h_j = h[j]\n",
    "        # calculate error\n",
    "        error = 5 - np.dot(w_i,h_j)\n",
    "        # increment MSE\n",
    "        mse += error**2\n",
    "        # gradients with L2 loss\n",
    "        # dictionary values are updated in place\n",
    "        h_j -= step_size.value*(-2*error*w_i + 2.0*reg.value*h_j)\n",
    "        w_i -= step_size.value*(-2*error*h_j + 2.0*reg.value*w_i)\n",
    "        # increment num updates\n",
    "        n_updates_acc += 1\n",
    "    # must massage results in something that will return properly\n",
    "    output = {}\n",
    "    for row_index in w:\n",
    "        output[('W', row_index)] = (row_index, w[row_index])\n",
    "    for col_index in h:\n",
    "        output[('H', col_index)] = (col_index, h[col_index])\n",
    "    # return iterator of updated W and H\n",
    "    return tuple((output.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "    .setAppName(\"ALSMatrixFactorization\")\\\n",
    "    .setMaster(\"local[*]\")\n",
    "sc = SparkContext(appName = \"SparseMatrixFactorization\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PARAMETERS FOR MF'''\n",
    "# number of workers\n",
    "cpus = sc.defaultParallelism\n",
    "# number of latent features\n",
    "k = 10\n",
    "# max iterations; 1 epoch ~= cpus\n",
    "max_iters = 100\n",
    "# mses list\n",
    "mses = []\n",
    "# step size parameters\n",
    "# decreasing beta will cause step size to decrease more slowly\n",
    "# increasing tau will make step size smaller overall\n",
    "beta = -0.8\n",
    "tau = 5500\n",
    "# regularization parameter\n",
    "reg = sc.broadcast(0.02)\n",
    "# this accumulator was for checking parameter tuning originally\n",
    "nanchecker = sc.accumulator(0)\n",
    "\n",
    "'''LOAD DATA'''\n",
    "# point to S3 bucket\n",
    "files = sc.textFile('s3a://github-recommender/utility/*txt', cpus)\n",
    "# parse text files into RDD of tuples that represent position in matrix with 1\n",
    "# minus 1 so indices are 0-indexed\n",
    "v = files.map(\n",
    "    lambda x: (int(x.split(\",\")[0])-1, int(x.split(\",\")[1])-1)).distinct() # (userId-1, itemId-1)\n",
    "\n",
    "'''HALF PRECISION FLOATING POINT'''\n",
    "# 6.10352 × 10−5 (minimum positive normal)\n",
    "lower = 0.000610352\n",
    "upper = lower*10\n",
    "\n",
    "'''INITIALIZE W AND H'''\n",
    "# initialize W\n",
    "# first map gets tuples of (i, 1)\n",
    "# reduceByKey to get (unique row indices, number of 1's in that row)\n",
    "# second map create W with (i, row array))\n",
    "\n",
    "w = v.map(lambda x: (x[0],1))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .map(lambda x: (x[0], np.random.uniform(lower,upper,k).astype('float16'))) # (userId, random arr)\n",
    "\n",
    "# initialize H\n",
    "# first map gets tuples of (j, 1)\n",
    "# reduceByKey to get (unique column indices, number of 1's in that column)\n",
    "# second map create H with (j, column array))\n",
    "h = v.map(lambda x: (x[1],1))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .map(lambda x: (x[0], np.random.uniform(lower,upper,k).astype('float16'))) # (itemId, random arr)\n",
    "\n",
    "\n",
    "'''CALCULATE ROWS, COLUMNS, BLOCK SIZE'''\n",
    "# number of users/rows\n",
    "n = w.count()\n",
    "# n = 4496672\n",
    "# number of columns/projects\n",
    "m = h.count()\n",
    "# m = 3253437\n",
    "# block size\n",
    "row_block_size = np.ceil(n*1./cpus)\n",
    "col_block_size = np.ceil(m*1./cpus)\n",
    "\n",
    "'''BLOCK V'''\n",
    "# separate V into 96 blocks based on row index\n",
    "blocked_v = v.keyBy(lambda x: assign_row_block(x[0])).partitionBy(cpus).persist() # (row_block, (userId, itemId))\n",
    "# \n",
    "\n",
    "'''STOCHASTIC GRADIENT DESCENT'''\n",
    "for i in range(max_iters):\n",
    "    # create accumulator for MSE\n",
    "    mse = sc.accumulator(0.0)\n",
    "    # create accumulator for number of updates per epoch\n",
    "    n_updates_acc = sc.accumulator(0)\n",
    "    # these accumulators are just for coalescing functions\n",
    "    w_accum = sc.accumulator(0)\n",
    "    h_accum = sc.accumulator(0)\n",
    "    # # step size is decreasing function of i (learning rate schedule)\n",
    "    # step_size = sc.broadcast(np.power(tau+i, beta))\n",
    "    # constant step size\n",
    "    step_size = sc.broadcast(0.001)\n",
    "    # randomly order strata of V\n",
    "    perms = np.random.permutation(cpus)\n",
    "    # create random strata (one sub-chunk of columns from each row block)\n",
    "    # perms[x[0]] = randomly permuted columns\n",
    "    # should make strata with rows from x[0] block and cols from perms[x[0]]\n",
    "    # filter out values that are in that row block but not in correct column block\n",
    "    filtered_v = blocked_v.filter(lambda x: perms[x[0]] == assign_col_block(x[1][1])).persist()\n",
    "    n_updates = filtered_v.count()\n",
    "    # W should have same block ids as V\n",
    "    blocked_w = w.keyBy(lambda x: assign_row_block(x[0]))  #(row_block, (userId, arr))\n",
    "    # block H with the row block id each strata should match with\n",
    "    blocked_h = h.keyBy(lambda x: np.where(perms == assign_col_block(x[0]))[0][0]) #(row_block, ())\n",
    "    # group the RDDs together\n",
    "    # returns [(row_block_id, (V_iter, W_iter, H_iter)), ...]\n",
    "    stratas = filtered_v.groupWith(blocked_w, blocked_h).partitionBy(cpus)\n",
    "    # run SGD on each block\n",
    "    # reduces by W and H so that each value is a list of tuples of newly updated W or H: (index, array)\n",
    "    w_h = stratas.mapPartitions(SGD).persist()\n",
    "    # unpersist filtered V, old W, and old H\n",
    "    filtered_v.unpersist()\n",
    "    w.unpersist()\n",
    "    h.unpersist()\n",
    "    w = w_h.filter(lambda x: x[0][0]=='W').map(lambda x: x[1]).persist()\n",
    "    h = w_h.filter(lambda x: x[0][0]=='H').map(lambda x: x[1]).persist()\n",
    "    # call action to actually compute this update!\n",
    "    w.foreach(coalesce_w)\n",
    "    h.foreach(coalesce_h)\n",
    "    # append current MSE\n",
    "    curr_mse = mse.value/n_updates_acc.value\n",
    "    print(\"MSE/update for {}-th iteration is: {}\".format(i, curr_mse))\n",
    "    mses.append(curr_mse)\n",
    "\n",
    "'''SAVE RESULTS'''\n",
    "# # turn RDD into matrix and save\n",
    "# w_values = np.zeros((n,k)).astype('float16')\n",
    "# h_values = np.zeros((k,m)).astype('float16')\n",
    "# w_accum = sc.accumulator(w_values, RowAccumulatorParam())\n",
    "# h_accum = sc.accumulator(h_values, ColAccumulatorParam())\n",
    "# w.foreach(coalesce_w)\n",
    "# h.foreach(coalesce_h)\n",
    "# w_accum.value.savetxt('s3a://github-recommender/output/w.txt')\n",
    "# h_accum.value.savetxt('s3a://github-recommender/output/h.txt')\n",
    "mses_rdd = sc.parallelize(mses)\n",
    "mses_rdd.saveAsTextFile('s3a://github-recommender/MSE')\n",
    "w.saveAsTextFile('s3a://github-recommender/W')\n",
    "h.saveAsTextFile('s3a://github-recommender/H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
